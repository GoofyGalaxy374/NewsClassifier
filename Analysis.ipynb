{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Classifying news articles as business or political, based on their title </h1>\n",
    "\n",
    "<h3> The goal of this notebook is to explore how one could use Tf-Idf and/or cosine similarity to classify news articles as business or political articles.</h3>\n",
    "\n",
    "<p> The Notebook is structured in the following manner: </p>\n",
    "<ol>\n",
    "<li> Data Cleaning </li> \n",
    "<li> Data Analysis </li> \n",
    "<li> Model Building </li> \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> Data cleaning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before we begin, we have to import all of our needed libraries. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as scp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we're ready, let's load the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Category</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As Democrats try to hold on in November, it’s ...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Dizzying week for Trump’s legal issues</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nancy Pelosi did what Donald Trump failed to d...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Trump told us all *exactly* what he was...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>See why Republicans are trying to get you to f...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Category Source\n",
       "0  As Democrats try to hold on in November, it’s ...  Politics    CNN\n",
       "1           A Dizzying week for Trump’s legal issues  Politics    CNN\n",
       "2  Nancy Pelosi did what Donald Trump failed to d...  Politics    CNN\n",
       "3  Donald Trump told us all *exactly* what he was...  Politics    CNN\n",
       "4  See why Republicans are trying to get you to f...  Politics    CNN"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDataset = pd.read_csv('./ArticleTitlesCategoryDataset.csv')\n",
    "newsDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Alright, the dataset is loaded successfully, now let's clean the data a bit. We'll do the following: </p>\n",
    "<ol>\n",
    "    <li> Turn all the contraction words into proper words (don't -> do not, I'd -> I would). </li>\n",
    "    <li> Remove all the punctuation from the title. </li>\n",
    "    <li> Tokenize the title. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnglishStopwords = set(stopwords.words('english'))\n",
    "\n",
    "Titles = newsDataset['Title']\n",
    "CleanedTitles = []\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "for title in Titles:\n",
    "    NoContractionsTitle = contractions.fix(title)\n",
    "    NoPunctuationTitle = NoContractionsTitle.translate(str.maketrans('','', string.punctuation)) \n",
    "    TokenizedTitle = word_tokenize(NoPunctuationTitle) \n",
    "    \n",
    "    cleanTitle = []\n",
    "    for word in TokenizedTitle:\n",
    "        if word not in EnglishStopwords:\n",
    "            LemmatizedVerb = Lemmatizer.lemmatize(word.lower(), pos='v')\n",
    "            LemmatizedNoun = Lemmatizer.lemmatize(LemmatizedVerb, pos='n')\n",
    "            if re.match('\\W+|\\d+', LemmatizedNoun) == None: \n",
    "                cleanTitle.append(LemmatizedNoun)\n",
    "                \n",
    "    cleanTitle = \" \".join(cleanTitle)\n",
    "    \n",
    "    CleanedTitles.append(cleanTitle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanTitle</th>\n",
       "      <th>Category</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a democrat try hold november pete buttigieg de...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a dizzy week trump legal issue</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nancy pelosi donald trump fail january</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donald trump tell u exactly go november</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see republican try get focus wokeness</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          CleanTitle  Category Source\n",
       "0  a democrat try hold november pete buttigieg de...  Politics    CNN\n",
       "1                     a dizzy week trump legal issue  Politics    CNN\n",
       "2             nancy pelosi donald trump fail january  Politics    CNN\n",
       "3            donald trump tell u exactly go november  Politics    CNN\n",
       "4              see republican try get focus wokeness  Politics    CNN"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanedDataset = pd.DataFrame(columns=['CleanTitle', 'Category', 'Source'])\n",
    "CleanedDataset['CleanTitle'] = CleanedTitles\n",
    "CleanedDataset['Category'] = newsDataset['Category']\n",
    "CleanedDataset['Source'] = newsDataset['Source']\n",
    "\n",
    "CleanedDataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Great ! We now have a clean titles dataset ! We can now proceed to the data analysis part!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> Data Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's find the most common words in the titles of political articles. </p>\n",
    "<p> First, we'll filter the data by the politics category. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanTitle</th>\n",
       "      <th>Category</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a democrat try hold november pete buttigieg de...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a dizzy week trump legal issue</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nancy pelosi donald trump fail january</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donald trump tell u exactly go november</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see republican try get focus wokeness</td>\n",
       "      <td>Politics</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          CleanTitle  Category Source\n",
       "0  a democrat try hold november pete buttigieg de...  Politics    CNN\n",
       "1                     a dizzy week trump legal issue  Politics    CNN\n",
       "2             nancy pelosi donald trump fail january  Politics    CNN\n",
       "3            donald trump tell u exactly go november  Politics    CNN\n",
       "4              see republican try get focus wokeness  Politics    CNN"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PoliticalArticles = CleanedDataset[CleanedDataset['Category'] == 'Politics']\n",
    "PoliticalArticles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have the political articles only, we can proceed with the extraction of the most common words in the titles. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 10),\n",
       " ('senate', 5),\n",
       " ('say', 5),\n",
       " ('democrat', 4),\n",
       " ('campaign', 4),\n",
       " ('u', 4),\n",
       " ('republican', 4),\n",
       " ('control', 4),\n",
       " ('warn', 4),\n",
       " ('biden', 4)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Titles = \"\".join(PoliticalArticles['CleanTitle'])\n",
    "TitlesTokenized = word_tokenize(Titles)\n",
    "fdist = FreqDist(TitlesTokenized)\n",
    "PoliticalMostCommonWords = fdist.most_common(10)\n",
    "PoliticalMostCommonWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We see that \"trump\", \"senate\", \"say\", \"democrat\", and \"campaign\" are some of the most common words in the political articles' titles. We could interpret those as a political article indicator.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Let's do the same for the business articles! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanTitle</th>\n",
       "      <th>Category</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>trevor milton founder nikola find guilty fraud</td>\n",
       "      <td>Business</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>more french gas station least one fuel</td>\n",
       "      <td>Business</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>these retail chain may survive recession</td>\n",
       "      <td>Business</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>it scary time hollywood but horror studio behi...</td>\n",
       "      <td>Business</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>last chance lock nearly return save</td>\n",
       "      <td>Business</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           CleanTitle  Category Source\n",
       "59     trevor milton founder nikola find guilty fraud  Business    CNN\n",
       "60             more french gas station least one fuel  Business    CNN\n",
       "61           these retail chain may survive recession  Business    CNN\n",
       "62  it scary time hollywood but horror studio behi...  Business    CNN\n",
       "63                last chance lock nearly return save  Business    CNN"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BusinessArticles = CleanedDataset[CleanedDataset['Category'] == 'Business']\n",
    "BusinessArticles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cut', 6),\n",
       " ('feed', 5),\n",
       " ('time', 4),\n",
       " ('warn', 4),\n",
       " ('go', 4),\n",
       " ('truss', 4),\n",
       " ('end', 3),\n",
       " ('lose', 3),\n",
       " ('largest', 3),\n",
       " ('first', 3)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BusinessTitles = \"\".join(BusinessArticles['CleanTitle'])\n",
    "BusinessTitlesTokenized = word_tokenize(BusinessTitles)\n",
    "fdist = FreqDist(BusinessTitlesTokenized)\n",
    "BusinessMostCommonWords = fdist.most_common(10)\n",
    "BusinessMostCommonWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Again, we see that \"cut\", \"feed\", \"time\", \"warn\", and \"go\" are some of the most common words in the business articles' titles. We could interpret those as a business article indicator.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> Model Building </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>What we could begin with is to find the most meaningful words in an article's title, using TF-IDF(term freuqnecy - document inverse frequency). </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The formula for the term frequency will tell us how frequent a certain term is in the article title that we're looking at. The formua is the following: </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ TF(t,d) = \\frac{t}{d} $, where $\\bold{t}$ is the occurence of the term we're looking for in a said article title, and $\\bold{d}$ is the amount of words in the target article title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The next thing we've got to do is lay out the formula for the Inverse-document frequency (IDF). It tells us how common(doesn't carry much meaning) or rare(it carries a lot of meaning/it's specific) a certain word is in a given article title. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ IDF(n,d) = \\log(\\frac{n}{d})$, where $\\bold{n}$ is the amount of articles and $\\bold{d}$ is the amount of times the term is seen across all article titles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The way we'll get a quantifier as to how relevant a certain word is to a certain article title is to use both TF and IDF together. We'll multiply them, that way we'll have a formula that looks like that: </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ TF-IDF = TF(t,d) * IDF(n,d) = \\frac{t}{d} * \\log(\\frac{n}{d})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The way we'll quantify if a word is relevant to the certain article title or not is the following:</p>\n",
    "<ol>\n",
    "<li>if TF-IDF returns a high number, then that will mean that the word is relevant and vice-versa.</li> \n",
    "<li>A low TF score would mean that the target term/word is not that frequent. On the other hand, a high TF score would mean that the word is very frequent in a certain text/article. </li>\n",
    "<li> In the case of interpreting an IDF score, a low score would mean higher relevance of the word with respect to the given text/article titles, and a high score would indicate the opposite. </li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next we'll create two TfIdfVectorizers, using nltk - one for the political articles and one for the business articles. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicalVectorizer = TfidfVectorizer()\n",
    "political_TFIDF_matrix = politicalVectorizer.fit_transform(PoliticalArticles['CleanTitle']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abortion</th>\n",
       "      <th>activist</th>\n",
       "      <th>admins</th>\n",
       "      <th>afraid</th>\n",
       "      <th>ag</th>\n",
       "      <th>agent</th>\n",
       "      <th>ally</th>\n",
       "      <th>ambassador</th>\n",
       "      <th>aoc</th>\n",
       "      <th>apply</th>\n",
       "      <th>...</th>\n",
       "      <th>why</th>\n",
       "      <th>win</th>\n",
       "      <th>wisconsin</th>\n",
       "      <th>wise</th>\n",
       "      <th>without</th>\n",
       "      <th>witness</th>\n",
       "      <th>wokeness</th>\n",
       "      <th>would</th>\n",
       "      <th>ye</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.459044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.308459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.300610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 353 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abortion  activist  admins  afraid   ag  agent  ally  ambassador  aoc  \\\n",
       "0  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "1  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "2  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "3  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "4  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "5  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "6  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "7  0.000000       0.0     0.0     0.0  0.0    0.0   0.0    0.349662  0.0   \n",
       "8  0.308459       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "9  0.300610       0.0     0.0     0.0  0.0    0.0   0.0    0.000000  0.0   \n",
       "\n",
       "   apply  ...       why       win  wisconsin  wise  without  witness  \\\n",
       "0    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "1    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "2    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "3    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "4    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "5    0.0  ...  0.424146  0.000000        0.0   0.0      0.0      0.0   \n",
       "6    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "7    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "8    0.0  ...  0.000000  0.308459        0.0   0.0      0.0      0.0   \n",
       "9    0.0  ...  0.000000  0.000000        0.0   0.0      0.0      0.0   \n",
       "\n",
       "   wokeness  would   ye      zero  \n",
       "0  0.000000    0.0  0.0  0.000000  \n",
       "1  0.000000    0.0  0.0  0.000000  \n",
       "2  0.000000    0.0  0.0  0.000000  \n",
       "3  0.000000    0.0  0.0  0.000000  \n",
       "4  0.459044    0.0  0.0  0.000000  \n",
       "5  0.000000    0.0  0.0  0.000000  \n",
       "6  0.000000    0.0  0.0  0.000000  \n",
       "7  0.000000    0.0  0.0  0.000000  \n",
       "8  0.000000    0.0  0.0  0.000000  \n",
       "9  0.000000    0.0  0.0  0.331114  \n",
       "\n",
       "[10 rows x 353 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_TFIDF_df = pd.DataFrame(political_TFIDF_matrix, columns=politicalVectorizer.get_feature_names())\n",
    "political_TFIDF_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's do the same for the business articles.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businessVectorizer = TfidfVectorizer()\n",
    "businessArticlesMatrix = businessVectorizer.fit_transform(BusinessArticles['CleanTitle']).toarray()\n",
    "businessArticlesMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>academic</th>\n",
       "      <th>adhesive</th>\n",
       "      <th>adsupported</th>\n",
       "      <th>african</th>\n",
       "      <th>ai</th>\n",
       "      <th>ambani</th>\n",
       "      <th>america</th>\n",
       "      <th>amid</th>\n",
       "      <th>announce</th>\n",
       "      <th>another</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>worry</th>\n",
       "      <th>worth</th>\n",
       "      <th>xi</th>\n",
       "      <th>year</th>\n",
       "      <th>yen</th>\n",
       "      <th>yet</th>\n",
       "      <th>young</th>\n",
       "      <th>zeroemissions</th>\n",
       "      <th>zte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.345836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.460466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 360 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   academic  adhesive  adsupported  african   ai  ambani   america  amid  \\\n",
       "0  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "1  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "2  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "3  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "4  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "5  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "6  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "7  0.345836       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "8  0.000000       0.0          0.0      0.0  0.0     0.0  0.460466   0.0   \n",
       "9  0.000000       0.0          0.0      0.0  0.0     0.0  0.000000   0.0   \n",
       "\n",
       "   announce  another  ...     world  worry  worth   xi  year  yen  yet  young  \\\n",
       "0       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "1       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "2       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "3       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "4       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "5       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "6       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "7       0.0      0.0  ...  0.345836    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "8       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "9       0.0      0.0  ...  0.000000    0.0    0.0  0.0   0.0  0.0  0.0    0.0   \n",
       "\n",
       "   zeroemissions  zte  \n",
       "0            0.0  0.0  \n",
       "1            0.0  0.0  \n",
       "2            0.0  0.0  \n",
       "3            0.0  0.0  \n",
       "4            0.0  0.0  \n",
       "5            0.0  0.0  \n",
       "6            0.0  0.0  \n",
       "7            0.0  0.0  \n",
       "8            0.0  0.0  \n",
       "9            0.0  0.0  \n",
       "\n",
       "[10 rows x 360 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_TFIDF_df = pd.DataFrame(businessArticlesMatrix, columns = businessVectorizer.get_feature_names())\n",
    "business_TFIDF_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we've seen what is happening \"behind the scenes\", let's get to the real deal. We'll create a pipeline that'll look like this: tf-idf vectorization -> multinomial Naive Bayes classifier.</p>\n",
    "\n",
    "<p>As to why I picked multinomial naive bayes for the model, you can read the scikit-learn documentation about it <a href='https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes'>here</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First, we'll split our dataset like this: 25% for testing and 75% for training. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_test, y_train, x_test = train_test_split(CleanedDataset['CleanTitle'], CleanedDataset['Category'], train_size=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we'll create our pipeline. We'll first run the data through the Tf-Idf Vectorizer, then we'll pass that data to the multinomial naive bayes algorithm. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Here we'll train our model on our train data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now that we have a model, it's time for us to test it! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_news_labels = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(x_test, predicted_news_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It seems to work, although I suppose that accuracy is due to our model overfittng on the data. </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
